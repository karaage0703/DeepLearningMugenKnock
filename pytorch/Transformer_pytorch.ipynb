{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Transformer_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoyoyo-yo/DeepLearningMugenKnock/blob/master/Scripts_NLP/pytorch/Transformer_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0H8bE7gkhEV",
        "colab_type": "text"
      },
      "source": [
        "# Transformer English - Franch\n",
        "\n",
        "元論文 : Attention if All You Need https://arxiv.org/abs/1706.03762 (2017)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIRpEHgCkoE0",
        "colab_type": "code",
        "outputId": "7fff570c-d5ff-4fe0-f7f4-247a644a091b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "!pip install numpy matplotlib opencv-python torch torchvision torchsummary pandas easydict"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.6.0+cu101)\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.3)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.6/dist-packages (1.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6j-kiToPUyo",
        "colab_type": "text"
      },
      "source": [
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sjoKf_PPQdl",
        "colab_type": "code",
        "outputId": "12cf33ca-322d-43bd-8f8b-d2a2ff905fb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "!wget https://www.manythings.org/anki/fra-eng.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-10 07:09:08--  https://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.24.108.196, 104.24.109.196, 2606:4700:3033::6818:6dc4, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.24.108.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5982778 (5.7M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip.2’\n",
            "\n",
            "\rfra-eng.zip.2         0%[                    ]       0  --.-KB/s               \rfra-eng.zip.2        15%[==>                 ] 884.49K  3.71MB/s               \rfra-eng.zip.2       100%[===================>]   5.71M  16.1MB/s    in 0.4s    \n",
            "\n",
            "2020-05-10 07:09:08 (16.1 MB/s) - ‘fra-eng.zip.2’ saved [5982778/5982778]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZlRB8duPQjN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "\n",
        "def load_ds():\n",
        "    #\n",
        "    _chars = '!?/.'\n",
        "    corpus1 = []\n",
        "    corpus2 = []\n",
        "\n",
        "    data1 = []\n",
        "    data2 = []\n",
        "\n",
        "    with zipfile.ZipFile('fra-eng.zip') as z:\n",
        "        with z.open('fra.txt') as f:\n",
        "            lines = f.readlines()\n",
        "            for x in lines[:100]:\n",
        "                lang1, lang2, _ = [y.rstrip() for y in x.decode('utf-8').split('\\t')]\n",
        "                corpus1 += [w.rstrip(_chars) for w in lang1.split(' ')] #list(set(corpus1) | set(list(lang1.split(' '))))\n",
        "                corpus2 += [w.rstrip(_chars).rstrip('\\u202f') for w in lang2.split(' ')] #list(set(corpus2) | set(list(lang2.split(' '))))\n",
        "                #print([y.rstrip() for y in x.decode('utf-8').split('\\t')])\n",
        "\n",
        "            corpus1 = list(set(corpus1)) # drop duplicate\n",
        "            corpus1.sort()\n",
        "            corpus1 = ['<SOS>', '<EOS>', '<UNKNOWN>'] + list(_chars) + corpus1\n",
        "            corpus2 = list(set(corpus2)) # drop duplicate\n",
        "            corpus2.sort()\n",
        "            corpus2 = ['<SOS>', '<EOS>', '<UNKNOWN>'] + list(_chars) + corpus2\n",
        "\n",
        "            for x in lines[:100]:\n",
        "                lang1, lang2, _ = [y.rstrip() for y in x.decode('utf-8').split('\\t')]\n",
        "                data1 += [[corpus1.index('<SOS>')] + [corpus1.index(w.rstrip(_chars)) for w in lang1.split(' ')] + [corpus1.index('<EOS>')]]\n",
        "                data2 += [[corpus2.index('<SOS>')] + [corpus2.index(w.rstrip(_chars).rstrip('\\u202f')) for w in lang2.split(' ')] + [corpus2.index('<EOS>')]]\n",
        "\n",
        "    return {'corpus1' : corpus1, 'corpus2' : corpus2, 'data1' : data1, 'data2' : data2} \n",
        "\n",
        "data_dict = load_ds()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ64UhTzkhEW",
        "colab_type": "text"
      },
      "source": [
        "## Import and Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JWBAlBskhEX",
        "colab_type": "code",
        "outputId": "15b125ef-0864-4294-a3d1-7c46fc985635",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "import argparse\n",
        "from pprint import pprint\n",
        "\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "from easydict import EasyDict\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "#---\n",
        "# config\n",
        "#---\n",
        "cfg = EasyDict()\n",
        "\n",
        "cfg.CORPUS1_NUM = len(data_dict['corpus1'])\n",
        "cfg.CORPUS2_NUM =  len(data_dict['corpus2'])\n",
        "\n",
        "# Seq2seq config\n",
        "cfg.SEQ2SEQ_MAX_LENGTH = 1000 # decoder max output length\n",
        "cfg.SEQ2SEQ_TRAIN_FORCE_PROB = 0.5 # train input is forced to gt with this probability\n",
        "cfg.SEQ2SEQ_NEXT_WORD_SELECTION = 'prob' # prob, argmax\n",
        "cfg.SEQ2SEQ_RNN_DIM = 512\n",
        "cfg.SEQ2SEQ_USE_RNN_BD = True # use bidirectional RNN\n",
        "\n",
        "cfg.SEQ2SEQ_E_ATTENTION = True\n",
        "cfg.SEQ2SEQ_E_ATTENTION_TIME = 6  # Hopping if > 1\n",
        "cfg.SEQ2SEQ_E_DIM = 64\n",
        "cfg.SEQ2SEQ_E_ATTENTION_DIM = 64\n",
        "cfg.SEQ2SEQ_E_DROPOUT = 0.2\n",
        "cfg.SEQ2SEQ_E_USE_SELF_ATTENTION = True # self attention of Encoder\n",
        "cfg.SEQ2SEQ_E_USE_SOURCE_TARGET_ATTENTION = True # use source target attention\n",
        "cfg.SEQ2SEQ_E_MULTIHEAD_ATTENTION_N = 8 # Multi head attention\n",
        "cfg.SEQ2SEQ_E_USE_FFN = True # Feed forward network\n",
        "cfg.SEQ2SEQ_E_FFN_DIM = 2048\n",
        "cfg.SEQ2SEQ_E_USE_PE = True # Positional encoding\n",
        "\n",
        "cfg.SEQ2SEQ_D_ATTENTION = True\n",
        "cfg.SEQ2SEQ_D_ATTENTION_TIME = 6  # Hopping if > 1\n",
        "cfg.SEQ2SEQ_D_DIM = 64\n",
        "cfg.SEQ2SEQ_D_ATTENTION_DIM = 64\n",
        "cfg.SEQ2SEQ_D_DROPOUT = 0.2\n",
        "cfg.SEQ2SEQ_D_USE_SELF_ATTENTIION = True # self attention of Decoder\n",
        "cfg.SEQ2SEQ_D_USE_SOURCE_TARGET_ATTENTION = True # use source target attention\n",
        "cfg.SEQ2SEQ_D_MULTIHEAD_ATTENTION_N = 8 # Multi head attention\n",
        "cfg.SEQ2SEQ_D_USE_FFN = True # Feed forward network\n",
        "cfg.SEQ2SEQ_D_FFN_DIM = 2048\n",
        "cfg.SEQ2SEQ_D_USE_PE = True # Positional encoding\n",
        "\n",
        "\n",
        "cfg.CHANNEL_AXIS = 1 # 1 ... [mb, c, h, w], 3 ... [mb, h, w, c]\n",
        "\n",
        "cfg.GPU = True\n",
        "cfg.DEVICE_TYPE = 'cuda' if cfg.GPU and torch.cuda.is_available() else 'cpu'\n",
        "cfg.DEVICE = torch.device(cfg.DEVICE_TYPE)\n",
        "\n",
        "# train\n",
        "cfg.TRAIN = EasyDict()\n",
        "cfg.TRAIN.DISPAY_ITERATION_INTERVAL = 50\n",
        "\n",
        "cfg.PREFIX = 'Seq2seq-Attention'\n",
        "cfg.TRAIN.MODEL_E_SAVE_PATH = 'models/' + cfg.PREFIX + '_E_{}.pt'\n",
        "cfg.TRAIN.MODEL_D_SAVE_PATH = 'models/' + cfg.PREFIX + '_D_{}.pt'\n",
        "cfg.TRAIN.MODEL_SAVE_INTERVAL = 200\n",
        "cfg.TRAIN.ITERATION = 10_000\n",
        "cfg.TRAIN.MINIBATCH = 32\n",
        "cfg.TRAIN.OPTIMIZER_E = torch.optim.Adam\n",
        "cfg.TRAIN.LEARNING_PARAMS_E = {'lr' : 0.01, 'betas' : (0., 0.9)}\n",
        "cfg.TRAIN.OPTIMIZER_D = torch.optim.Adam\n",
        "cfg.TRAIN.LEARNING_PARAMS_D = {'lr' : 0.01, 'betas' : (0., 0.9)}\n",
        "cfg.TRAIN.LOSS_FUNCTION = torch.nn.NLLLoss()\n",
        "\n",
        "cfg.TRAIN.DATA_PATH = '/content/drive/My Drive/Colab Notebooks/Dataset/train/images/'\n",
        "cfg.TRAIN.DATA_HORIZONTAL_FLIP = True # data augmentation : holizontal flip\n",
        "cfg.TRAIN.DATA_VERTICAL_FLIP = True # data augmentation : vertical flip\n",
        "cfg.TRAIN.DATA_ROTATION = 1 # data augmentation : rotation False, or integer\n",
        "\n",
        "cfg.TRAIN.LEARNING_PROCESS_RESULT_SAVE = True\n",
        "cfg.TRAIN.LEARNING_PROCESS_RESULT_INTERVAL = 200\n",
        "cfg.TRAIN.LEARNING_PROCESS_RESULT_IMAGE_PATH = 'result/' + cfg.PREFIX + '_result_{}.jpg'\n",
        "cfg.TRAIN.LEARNING_PROCESS_RESULT_LOSS_PATH = 'result/' + cfg.PREFIX + '_loss.txt'\n",
        "\n",
        "\n",
        "# test\n",
        "cfg.TEST = EasyDict()\n",
        "cfg.TEST.MODEL_E_PATH = cfg.TRAIN.MODEL_E_SAVE_PATH.format('final')\n",
        "cfg.TEST.MODEL_D_PATH = cfg.TRAIN.MODEL_D_SAVE_PATH.format('final')\n",
        "cfg.TEST.DATA_PATH = '/content/drive/My Drive/Colab Notebooks/Dataset/test/images/'\n",
        "cfg.TEST.MINIBATCH = 10\n",
        "cfg.TEST.ITERATION = 2\n",
        "cfg.TEST.RESULT_SAVE = False\n",
        "cfg.TEST.RESULT_IMAGE_PATH = 'result/' + cfg.PREFIX + '_result_{}.jpg'\n",
        "\n",
        "# random seed\n",
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "# make model save directory\n",
        "def make_dir(path):\n",
        "    if '/' in path:\n",
        "        model_save_dir = '/'.join(path.split('/')[:-1])\n",
        "        os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "make_dir(cfg.TRAIN.MODEL_D_SAVE_PATH)\n",
        "make_dir(cfg.TRAIN.LEARNING_PROCESS_RESULT_IMAGE_PATH)\n",
        "make_dir(cfg.TRAIN.LEARNING_PROCESS_RESULT_LOSS_PATH)\n",
        "\n",
        "pprint(cfg)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'CHANNEL_AXIS': 1,\n",
            " 'CORPUS1_NUM': 64,\n",
            " 'CORPUS2_NUM': 132,\n",
            " 'DEVICE': device(type='cuda'),\n",
            " 'DEVICE_TYPE': 'cuda',\n",
            " 'GPU': True,\n",
            " 'PREFIX': 'Seq2seq-Attention',\n",
            " 'SEQ2SEQ_D_ATTENTION': True,\n",
            " 'SEQ2SEQ_D_ATTENTION_DIM': 64,\n",
            " 'SEQ2SEQ_D_ATTENTION_TIME': 6,\n",
            " 'SEQ2SEQ_D_DIM': 64,\n",
            " 'SEQ2SEQ_D_DROPOUT': 0.2,\n",
            " 'SEQ2SEQ_D_FFN_DIM': 2048,\n",
            " 'SEQ2SEQ_D_MULTIHEAD_ATTENTION_N': 8,\n",
            " 'SEQ2SEQ_D_USE_FFN': True,\n",
            " 'SEQ2SEQ_D_USE_PE': True,\n",
            " 'SEQ2SEQ_D_USE_SELF_ATTENTIION': True,\n",
            " 'SEQ2SEQ_D_USE_SOURCE_TARGET_ATTENTION': True,\n",
            " 'SEQ2SEQ_E_ATTENTION': True,\n",
            " 'SEQ2SEQ_E_ATTENTION_DIM': 64,\n",
            " 'SEQ2SEQ_E_ATTENTION_TIME': 6,\n",
            " 'SEQ2SEQ_E_DIM': 64,\n",
            " 'SEQ2SEQ_E_DROPOUT': 0.2,\n",
            " 'SEQ2SEQ_E_FFN_DIM': 2048,\n",
            " 'SEQ2SEQ_E_MULTIHEAD_ATTENTION_N': 8,\n",
            " 'SEQ2SEQ_E_USE_FFN': True,\n",
            " 'SEQ2SEQ_E_USE_PE': True,\n",
            " 'SEQ2SEQ_E_USE_SELF_ATTENTION': True,\n",
            " 'SEQ2SEQ_E_USE_SOURCE_TARGET_ATTENTION': True,\n",
            " 'SEQ2SEQ_MAX_LENGTH': 1000,\n",
            " 'SEQ2SEQ_NEXT_WORD_SELECTION': 'prob',\n",
            " 'SEQ2SEQ_RNN_DIM': 512,\n",
            " 'SEQ2SEQ_TRAIN_FORCE_PROB': 0.5,\n",
            " 'SEQ2SEQ_USE_RNN_BD': True,\n",
            " 'TEST': {'DATA_PATH': '/content/drive/My Drive/Colab '\n",
            "                       'Notebooks/Dataset/test/images/',\n",
            "          'ITERATION': 2,\n",
            "          'MINIBATCH': 10,\n",
            "          'MODEL_D_PATH': 'models/Seq2seq-Attention_D_final.pt',\n",
            "          'MODEL_E_PATH': 'models/Seq2seq-Attention_E_final.pt',\n",
            "          'RESULT_IMAGE_PATH': 'result/Seq2seq-Attention_result_{}.jpg',\n",
            "          'RESULT_SAVE': False},\n",
            " 'TRAIN': {'DATA_HORIZONTAL_FLIP': True,\n",
            "           'DATA_PATH': '/content/drive/My Drive/Colab '\n",
            "                        'Notebooks/Dataset/train/images/',\n",
            "           'DATA_ROTATION': 1,\n",
            "           'DATA_VERTICAL_FLIP': True,\n",
            "           'DISPAY_ITERATION_INTERVAL': 50,\n",
            "           'ITERATION': 10000,\n",
            "           'LEARNING_PARAMS_D': {'betas': [0.0, 0.9], 'lr': 0.01},\n",
            "           'LEARNING_PARAMS_E': {'betas': [0.0, 0.9], 'lr': 0.01},\n",
            "           'LEARNING_PROCESS_RESULT_IMAGE_PATH': 'result/Seq2seq-Attention_result_{}.jpg',\n",
            "           'LEARNING_PROCESS_RESULT_INTERVAL': 200,\n",
            "           'LEARNING_PROCESS_RESULT_LOSS_PATH': 'result/Seq2seq-Attention_loss.txt',\n",
            "           'LEARNING_PROCESS_RESULT_SAVE': True,\n",
            "           'LOSS_FUNCTION': NLLLoss(),\n",
            "           'MINIBATCH': 32,\n",
            "           'MODEL_D_SAVE_PATH': 'models/Seq2seq-Attention_D_{}.pt',\n",
            "           'MODEL_E_SAVE_PATH': 'models/Seq2seq-Attention_E_{}.pt',\n",
            "           'MODEL_SAVE_INTERVAL': 200,\n",
            "           'OPTIMIZER_D': <class 'torch.optim.adam.Adam'>,\n",
            "           'OPTIMIZER_E': <class 'torch.optim.adam.Adam'>}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfhZkkxckhEZ",
        "colab_type": "text"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHSZpaajkhEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Reshape(torch.nn.Module):\n",
        "    def __init__(self, shape):\n",
        "        super(Reshape, self).__init__()\n",
        "        self.shape = shape\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x.reshape(self.shape)\n",
        "\n",
        "class Permute(torch.nn.Module):\n",
        "    def __init__(self, *args):\n",
        "        super(Permute, self).__init__()\n",
        "        self.shape = args\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x.permute(self.shape)\n",
        "\n",
        "\n",
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, rnn_dim=64, rnn_hidden_size=1, attention_dim=64, max_length=100, \n",
        "        dropout_p=0.1, attention_time=1, use_source_target_attention=False,\n",
        "        use_self_attention=False, multiHead_attention_num=1, use_FFN=False, FFN_dim=2048, use_PE=False, use_bd=False):\n",
        "    \n",
        "        super(Encoder, self).__init__()\n",
        "        self.max_length = max_length\n",
        "        self.rnn_dim = rnn_dim\n",
        "        self.rnn_hidden_size = rnn_hidden_size\n",
        "\n",
        "        # Embedding\n",
        "        self.embedding = torch.nn.Embedding(input_dim, hidden_dim)\n",
        "\n",
        "        # Positional Encoding\n",
        "        if use_PE:\n",
        "            self.pe = PE()\n",
        "\n",
        "        # Attention\n",
        "        self.attentions = []\n",
        "        if attention_time > 0:\n",
        "            for i in range(attention_time):\n",
        "                # Self Attention\n",
        "                if use_self_attention:\n",
        "                    self.attentions.append(Attention(\n",
        "                        hidden_dim=hidden_dim, memory_dim=hidden_dim, attention_dim=attention_dim, output_dim=hidden_dim,\n",
        "                        dropout_p=dropout_p, max_length=max_length, self_attention=use_self_attention, head_num=multiHead_attention_num))\n",
        "\n",
        "                # Feed Forward Network\n",
        "                if use_FFN:\n",
        "                    self.attentions.append(FFN(dim=FFN_dim, d_model=hidden_dim, dropout_p=dropout_p))\n",
        "\n",
        "        self.attentions = torch.nn.ModuleList(self.attentions)\n",
        "\n",
        "        # output GRU\n",
        "        self.gru = torch.nn.GRU(hidden_dim, rnn_dim, bidirectional=use_bd)\n",
        "\n",
        "\n",
        "    def forward(self, x, hidden, x_memory):\n",
        "        # Embedding\n",
        "        x = self.embedding(x).view(1, 1, -1)\n",
        "        x_memory = self.embedding(x_memory).permute(1, 0, 2)\n",
        "        x_memory = x_memory.float()\n",
        "\n",
        "        # Positional Encoding\n",
        "        if hasattr(self, 'PE'):\n",
        "            x = self.pe(x)\n",
        "            x_memory = self.pe(x_memory)\n",
        "\n",
        "        # Attention\n",
        "        for layer in self.attentions:\n",
        "            x = layer(x, x_memory, x_memory)\n",
        "\n",
        "        # RNN\n",
        "        x, hidden = self.gru(x, hidden)\n",
        "        return x, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(self.rnn_hidden_size, 1, self.rnn_dim).to(cfg.DEVICE)\n",
        "\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, output_dim, hidden_dim=64, rnn_dim=64, attention_dim=64, dropout_p=0.1,\n",
        "        attention_time=1, max_length=100, use_source_target_attention=False, use_self_attention=False,\n",
        "        multiHead_attention_num=2, use_FFN=False, FFN_dim=2048, use_PE=False, use_bd=False):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Embedding\n",
        "        self.input_embedding = torch.nn.Embedding(output_dim, hidden_dim)\n",
        "        self.input_embedding_dropout = torch.nn.Dropout(dropout_p)\n",
        "\n",
        "        # Positional Encoding\n",
        "        if use_PE:\n",
        "            self.pe = PE()\n",
        "\n",
        "        # Attention\n",
        "        self.attentions = []\n",
        "        if attention_time > 0:\n",
        "            for i in range(attention_time):\n",
        "                # Self Attention\n",
        "                if use_self_attention:\n",
        "                    self.attentions.append(\n",
        "                        Attention(hidden_dim=hidden_dim, memory_dim=hidden_dim, attention_dim=attention_dim, output_dim=hidden_dim,\n",
        "                        dropout_p=dropout_p, max_length=max_length, self_attention=use_self_attention, head_num=multiHead_attention_num))\n",
        "                \n",
        "                # Source Target Attention\n",
        "                if use_source_target_attention:\n",
        "                    self.attentions.append(\n",
        "                        Attention(hidden_dim=hidden_dim, memory_dim=rnn_dim * (use_bd + 1), attention_dim=attention_dim, output_dim=hidden_dim,\n",
        "                        dropout_p=dropout_p, max_length=max_length, head_num=multiHead_attention_num))\n",
        "\n",
        "                # Feed Forward Network\n",
        "                if use_FFN:\n",
        "                    self.attentions.append(FFN(dim=FFN_dim, d_model=hidden_dim, dropout_p=dropout_p))\n",
        "\n",
        "        self.attentions = torch.nn.ModuleList(self.attentions)\n",
        "\n",
        "        # output GRU\n",
        "        self.gru = torch.nn.GRU(hidden_dim, rnn_dim, bidirectional=use_bd)\n",
        "        self.out = torch.nn.Sequential(\n",
        "            torch.nn.Linear(rnn_dim * (use_bd + 1), output_dim),\n",
        "            torch.nn.Softmax(dim=-1)\n",
        "        )\n",
        "    \n",
        "\n",
        "    def forward(self, x, hidden, x_memory_encoder, x_self_memory):\n",
        "        # Embedding\n",
        "        x = self.input_embedding(x)\n",
        "        x = self.input_embedding_dropout(x)\n",
        "\n",
        "        # Memory Embedding\n",
        "        x_self_memory = self.input_embedding(x_self_memory)#.permute(1, 0, 2)\n",
        "\n",
        "        # Positional Encoding\n",
        "        if hasattr(self, \"pe\"):\n",
        "            x = self.pe(x)\n",
        "            x_self_memory = self.pe(x_self_memory)\n",
        "\n",
        "        # Attention\n",
        "        for layer in self.attentions:\n",
        "            x = layer(x, x_memory_encoder, x_self_memory)\n",
        "\n",
        "        # output GRU\n",
        "        x, hidden = self.gru(x, hidden)\n",
        "        x = self.out(x[0])\n",
        "        return x, hidden\n",
        "\n",
        "\n",
        "\n",
        "class Attention(torch.nn.Module):\n",
        "    def __init__(self, hidden_dim, memory_dim, attention_dim, output_dim, dropout_p=0.1, max_length=100, head_num=1, self_attention=False):\n",
        "        super(Attention, self).__init__()\n",
        "        self.max_length = max_length\n",
        "        self.self_attention = self_attention\n",
        "\n",
        "        # Attention Query\n",
        "        self.Q = torch.nn.Sequential(\n",
        "            Reshape([1, -1]),\n",
        "            torch.nn.Linear(hidden_dim, attention_dim),\n",
        "            torch.nn.Dropout(dropout_p),\n",
        "            Reshape([1, 1, -1]),\n",
        "            Reshape([1, attention_dim // head_num, head_num]), # Multi head attention\n",
        "            Permute(2, 0, 1),\n",
        "        )\n",
        "        \n",
        "        # Attention Key\n",
        "        self.K = torch.nn.Sequential(\n",
        "            torch.nn.Linear(memory_dim, attention_dim),\n",
        "            torch.nn.Dropout(dropout_p),\n",
        "            Reshape([1, -1, attention_dim]),\n",
        "            Reshape([-1, attention_dim // head_num, head_num]), # Multi head attention\n",
        "            Permute(2, 1, 0)\n",
        "        )\n",
        "        \n",
        "        # Attetion Value\n",
        "        self.V = torch.nn.Sequential(\n",
        "            torch.nn.Linear(memory_dim, attention_dim),\n",
        "            torch.nn.Dropout(dropout_p),\n",
        "            Reshape([1, -1, attention_dim]),\n",
        "            Reshape([-1, attention_dim // head_num, head_num]), # Multi head attention\n",
        "            Permute(2, 0, 1),\n",
        "        )\n",
        "\n",
        "        self.out = torch.nn.Sequential(\n",
        "            torch.nn.Linear(attention_dim, output_dim),\n",
        "            torch.nn.Dropout(dropout_p)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, memory, memory2):\n",
        "        # get Query\n",
        "        Q = self.Q(x)\n",
        "        Q *= Q.size()[-1] ** -0.5 # scaled dot product\n",
        "\n",
        "        if self.self_attention:\n",
        "            memory = memory2\n",
        "\n",
        "        # memory transforme [mb(=1), length, dim] -> [length, dim]\n",
        "        if len(memory.size()) > 2:\n",
        "            memory = memory[0]\n",
        "        \n",
        "        # get Key\n",
        "        K = self.K(memory)\n",
        "\n",
        "        QK = torch.bmm(Q, K) # get Query and Key (= attention logits)\n",
        "\n",
        "        # masking attention weight\n",
        "        any_zero = memory.sum(dim=1)\n",
        "        pad_mask = torch.ones([1, 1, self.max_length]).to(cfg.DEVICE)\n",
        "        pad_mask[:, :, torch.nonzero(any_zero)] = 0\n",
        "\n",
        "        pad_mask = pad_mask[:, :, :QK.size()[-1]] # crop \n",
        "        QK += pad_mask * 1e-10\n",
        "        attention_weights = F.softmax(QK, dim=-1) # get attention weight\n",
        "        \n",
        "        # get Value\n",
        "        V = self.V(memory)\n",
        "        \n",
        "        # Attetion x Value\n",
        "        x = torch.bmm(attention_weights, V)\n",
        "\n",
        "        # Multi head -> one head\n",
        "        x = x.permute(1, 2, 0).reshape(1, 1, -1)\n",
        "        return self.out(x)\n",
        "\n",
        "\n",
        "class FFN(torch.nn.Module):\n",
        "    def __init__(self, dim, d_model, dropout_p=0.1):\n",
        "        super(FFN, self).__init__()\n",
        "\n",
        "        self.module = torch.nn.Sequential(\n",
        "            torch.nn.Linear(d_model, dim),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(dropout_p),\n",
        "            torch.nn.Linear(dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, memory_encoder, decoder):\n",
        "        return self.module(x)\n",
        "\n",
        "class PE(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PE, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        mb, pos, dim = x.size()\n",
        "        pe = np.zeros_like(x.detach().cpu().numpy())\n",
        "        pos_i, dim_i = np.meshgrid(np.arange(dim), np.arange(pos))\n",
        "        pe[..., 0::2] = np.sin(pos_i[..., 0::2] / (10000 ** (2 * dim_i[..., 0::2] / dim)))\n",
        "        pe[..., 1::2] = np.cos(pos_i[..., 1::2] / (10000 ** (2 * dim_i[..., 1::2] / dim)))\n",
        "        pe = torch.tensor(pe).to(cfg.DEVICE)\n",
        "        return x + pe\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWdUenQ8khEc",
        "colab_type": "text"
      },
      "source": [
        "## Utility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw9HD3tjkhEc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MInibatch_Generator():\n",
        "    def __init__(self, data_size, batch_size, shuffle=True):\n",
        "        self.data_size = data_size\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.mbi = 0 # index for iteration\n",
        "        self.inds = np.arange(data_size)\n",
        "        np.random.shuffle(self.inds)\n",
        "\n",
        "    def __call__(self):\n",
        "        if self.mbi + self.batch_size > self.data_size:\n",
        "            inds = self.inds[self.mbi:]\n",
        "            np.random.shuffle(self.inds)\n",
        "            inds = np.hstack((inds, self.inds[ : (self.batch_size - (self.data_size - self.mbi))]))\n",
        "            mbi = self.batch_size - (self.data_size - self.mbi)\n",
        "        else:\n",
        "            inds = self.inds[self.mbi : self.mbi + self.batch_size]\n",
        "            self.mbi += self.batch_size\n",
        "        return inds\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvsBhxJzkhEe",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "u93U8jihkhEe",
        "colab_type": "code",
        "outputId": "86931f5a-b769-4416-cf31-0d95452a9b8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        }
      },
      "source": [
        "# train\n",
        "def train():\n",
        "    # model\n",
        "    E = Encoder(\n",
        "        input_dim = cfg.CORPUS1_NUM, \n",
        "        hidden_dim = cfg.SEQ2SEQ_E_DIM,\n",
        "        attention_dim = cfg.SEQ2SEQ_E_ATTENTION_DIM,\n",
        "        rnn_dim = cfg.SEQ2SEQ_RNN_DIM,\n",
        "        rnn_hidden_size = cfg.SEQ2SEQ_USE_RNN_BD + 1,\n",
        "        use_bd = cfg.SEQ2SEQ_USE_RNN_BD,\n",
        "        dropout_p = cfg.SEQ2SEQ_E_DROPOUT,\n",
        "        attention_time = cfg.SEQ2SEQ_E_ATTENTION_TIME,\n",
        "        use_source_target_attention = cfg.SEQ2SEQ_E_USE_SOURCE_TARGET_ATTENTION,\n",
        "        use_self_attention = cfg.SEQ2SEQ_E_USE_SELF_ATTENTION,\n",
        "        multiHead_attention_num = cfg.SEQ2SEQ_E_MULTIHEAD_ATTENTION_N,\n",
        "        use_FFN = cfg.SEQ2SEQ_E_USE_FFN,\n",
        "        FFN_dim = cfg.SEQ2SEQ_E_FFN_DIM,\n",
        "        use_PE = cfg.SEQ2SEQ_E_USE_PE,\n",
        "        max_length = cfg.SEQ2SEQ_MAX_LENGTH\n",
        "        ).to(cfg.DEVICE) \n",
        "\n",
        "    D = Decoder(\n",
        "        output_dim = cfg.CORPUS2_NUM, \n",
        "        hidden_dim = cfg.SEQ2SEQ_E_DIM,\n",
        "        rnn_dim = cfg.SEQ2SEQ_RNN_DIM,\n",
        "        use_bd = cfg.SEQ2SEQ_USE_RNN_BD,\n",
        "        attention_dim = cfg.SEQ2SEQ_E_ATTENTION_DIM,\n",
        "        dropout_p = cfg.SEQ2SEQ_E_DROPOUT,\n",
        "        attention_time = cfg.SEQ2SEQ_E_ATTENTION_TIME,\n",
        "        use_source_target_attention = cfg.SEQ2SEQ_E_USE_SOURCE_TARGET_ATTENTION,\n",
        "        use_self_attention = cfg.SEQ2SEQ_E_USE_SELF_ATTENTION,\n",
        "        multiHead_attention_num = cfg.SEQ2SEQ_E_MULTIHEAD_ATTENTION_N,\n",
        "        use_FFN = cfg.SEQ2SEQ_E_USE_FFN,\n",
        "        FFN_dim = cfg.SEQ2SEQ_E_FFN_DIM,\n",
        "        use_PE = cfg.SEQ2SEQ_E_USE_PE,\n",
        "        max_length = cfg.SEQ2SEQ_MAX_LENGTH\n",
        "        ).to(cfg.DEVICE)\n",
        "\n",
        "    #summary(E, (cfg.INPUT_Z_DIM, 1, 1), device=cfg.DEVICE_TYPE)\n",
        "    #summary(D, (cfg.OUTPUT_CHANNEL, cfg.OUTPUT_HEIGHT, cfg.OUTPUT_WIDTH), device=cfg.DEVICE_TYPE)\n",
        "    \n",
        "    opt_E = cfg.TRAIN.OPTIMIZER_E(E.parameters(), **cfg.TRAIN.LEARNING_PARAMS_E)\n",
        "    opt_D = cfg.TRAIN.OPTIMIZER_D(D.parameters(), **cfg.TRAIN.LEARNING_PARAMS_D)\n",
        "\n",
        "    list_iter = []\n",
        "    list_loss = []\n",
        "    list_accuracy = []\n",
        "\n",
        "    #dataset = MyDataset(data_dict['data1'], data_dict['data2'])\n",
        "    #dataloader = torch.utils.data.DataLoader(dataset, batch_size=cfg.TRAIN.MINIBATCH, shuffle=True)\n",
        "\n",
        "    mb_gen = MInibatch_Generator(len(data_dict['data1']), cfg.TRAIN.MINIBATCH)\n",
        "\n",
        "    print('training start')\n",
        "    progres_bar = ''\n",
        "\n",
        "    Xs_train = data_dict['data1']\n",
        "    ts_train = data_dict['data2']\n",
        "\n",
        "    for i in range(cfg.TRAIN.ITERATION):\n",
        "        idxs = mb_gen()\n",
        "        loss = 0.\n",
        "        accuracy = 0.\n",
        "        total_len = 0.\n",
        "        _Xs = [Xs_train[idx] for idx in idxs]\n",
        "        _ts = [ts_train[idx] for idx in idxs]\n",
        "\n",
        "        # each iteration in minibatch\n",
        "        opt_E.zero_grad()\n",
        "        opt_D.zero_grad()\n",
        "\n",
        "        for mbi in range(cfg.TRAIN.MINIBATCH):\n",
        "            Xs = torch.tensor(_Xs[mbi]).reshape(-1, 1).to(cfg.DEVICE)\n",
        "            ts = torch.tensor(_ts[mbi]).reshape(-1, 1).to(cfg.DEVICE)\n",
        "        \n",
        "            xs_length = Xs.size()[0]\n",
        "            ts_length = ts.size()[0]\n",
        "            total_len += ts_length\n",
        "\n",
        "            # encode process\n",
        "            E_hidden = E.initHidden() # initialize encoder hidden\n",
        "            E_outputs = torch.zeros(cfg.SEQ2SEQ_MAX_LENGTH, cfg.SEQ2SEQ_RNN_DIM * (cfg.SEQ2SEQ_USE_RNN_BD + 1)).to(cfg.DEVICE)\n",
        "\n",
        "            for ei in range(xs_length):\n",
        "                E_output, E_hidden = E(Xs[ei], E_hidden, Xs)\n",
        "                E_outputs[ei] = E_output[0, 0]\n",
        "\n",
        "            # decode process\n",
        "            D_xs = ts[0].reshape(1, -1) # define decoder input\n",
        "            D_hidden = E_hidden # define decoder hidden\n",
        "            D_self_memory = D_xs\n",
        "            D_outputs = []\n",
        "\n",
        "            # define whethere if use teacher label for decoder input\n",
        "            use_teacher = True if np.random.random() < cfg.SEQ2SEQ_TRAIN_FORCE_PROB else False\n",
        "\n",
        "            for di in range(1, ts_length):\n",
        "                # decode\n",
        "                D_ys, D_hidden = D(D_xs, D_hidden, E_outputs, D_self_memory)\n",
        "\n",
        "                # add loss\n",
        "                loss += cfg.TRAIN.LOSS_FUNCTION(torch.log(D_ys), ts[di])\n",
        "\n",
        "                # count accuracy\n",
        "                if D_ys.argmax() == ts[di]:\n",
        "                    accuracy += 1.\n",
        "                \n",
        "                if cfg.SEQ2SEQ_NEXT_WORD_SELECTION == \"argmax\":\n",
        "                    topv, topi = D_ys.data.topk(1)\n",
        "\n",
        "                elif cfg.SEQ2SEQ_NEXT_WORD_SELECTION == \"prob\":\n",
        "                    topi = torch.multinomial(torch.max(D_ys, torch.zeros_like(D_ys)), 1)\n",
        "                \n",
        "                # define next decoder input\n",
        "                if use_teacher:\n",
        "                    D_xs = ts[di] # teacher forcing\n",
        "                else:\n",
        "                    D_xs = topi#.squeeze().detach()\n",
        "\n",
        "                D_xs = D_xs.reshape(1, -1)\n",
        "                D_self_memory = torch.cat([D_self_memory, D_xs])\n",
        "\n",
        "                D_outputs.append(topi.detach().cpu().numpy()[0])\n",
        "                     \n",
        "                # if EOS, finish training\n",
        "                #if D_xs.item() == data_dict['corpus2'].index('<EOS>'):\n",
        "                #    break\n",
        "\n",
        "        loss.backward()\n",
        "        opt_D.step()\n",
        "\n",
        "        _loss = loss.item() / cfg.TRAIN.MINIBATCH\n",
        "        _accuracy = accuracy / total_len\n",
        "\n",
        "        progres_bar += '|'\n",
        "        print('\\r' + 'Loss:{:.4f}, Accu:{:.4f} '.format(_loss, _accuracy) + progres_bar, end='')\n",
        "\n",
        "        if (i + 1) % 10 == 0:\n",
        "            progres_bar += str(i + 1)\n",
        "            print('\\r' + 'Loss:{:.4f}, Accu:{:.4f} '.format(_loss, _accuracy) + progres_bar, end='')\n",
        "\n",
        "            # save process result\n",
        "            if cfg.TRAIN.LEARNING_PROCESS_RESULT_SAVE:\n",
        "                list_iter.append(i + 1)\n",
        "                list_loss.append(_loss)\n",
        "                list_accuracy.append(_accuracy)\n",
        "\n",
        "        # display training state\n",
        "        if (i + 1) % cfg.TRAIN.DISPAY_ITERATION_INTERVAL == 0:\n",
        "            print('\\r' + ' ' * (len(progres_bar) + 50), end='')\n",
        "            print('\\rIter:{}, Loss:{:.4f}, Accu:{:.4f}'.format(i + 1, _loss, _accuracy))\n",
        "            progres_bar = ''\n",
        "\n",
        "        # save parameters\n",
        "        if (cfg.TRAIN.MODEL_SAVE_INTERVAL != False) and ((i + 1) % cfg.TRAIN.MODEL_SAVE_INTERVAL == 0):\n",
        "            E_save_path = cfg.TRAIN.MODEL_E_SAVE_PATH.format('iter{}'.format(i + 1))\n",
        "            D_save_path = cfg.TRAIN.MODEL_D_SAVE_PATH.format('iter{}'.format(i + 1))\n",
        "            torch.save(E.state_dict(), E_save_path)\n",
        "            torch.save(D.state_dict(), D_save_path)\n",
        "            print('save E >> {}, D >> {}'.format(E_save_path, D_save_path))\n",
        "\n",
        "        # save process result\n",
        "        if cfg.TRAIN.LEARNING_PROCESS_RESULT_SAVE and ((i + 1) % cfg.TRAIN.LEARNING_PROCESS_RESULT_INTERVAL == 0):\n",
        "            print('iter :', i + 1)\n",
        "            print(' - [input]', ' '.join([data_dict['corpus1'][x] for x in _Xs[0][1:-1]]))\n",
        "            print(' - [output]', ' '.join([data_dict['corpus2'][x] for x in D_outputs if x not in [0, 1, 2]]))\n",
        "            print(' - [gt]', ' '.join([data_dict['corpus2'][x] for x in _ts[0][1:-1]]))\n",
        "\n",
        "    E_save_path = cfg.TRAIN.MODEL_E_SAVE_PATH.format('final')\n",
        "    D_save_path = cfg.TRAIN.MODEL_D_SAVE_PATH.format('final')\n",
        "    torch.save(E.state_dict(), E_save_path)\n",
        "    torch.save(D.state_dict(), D_save_path)\n",
        "    print('final paramters were saved to E >> {}, D >> {}'.format(E_save_path, D_save_path))\n",
        "\n",
        "    if cfg.TRAIN.LEARNING_PROCESS_RESULT_SAVE:\n",
        "        f = open(cfg.TRAIN.LEARNING_PROCESS_RESULT_LOSS_PATH, 'w')\n",
        "        df = pd.DataFrame({'iteration' : list_iter, 'loss' : list_loss, 'accuracy' : list_accuracy})\n",
        "        df.to_csv(cfg.TRAIN.LEARNING_PROCESS_RESULT_LOSS_PATH, index=False)\n",
        "        print('loss was saved to >> {}'.format(cfg.TRAIN.LEARNING_PROCESS_RESULT_LOSS_PATH))\n",
        "\n",
        "train()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero(Tensor input, *, Tensor out)\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(Tensor input, *, bool as_tuple)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter:50, Loss:11.1560, Accu:0.2406\n",
            "Iter:100, Loss:13.2247, Accu:0.1267\n",
            "Iter:150, Loss:10.9344, Accu:0.2370\n",
            "Iter:200, Loss:11.7231, Accu:0.2222\n",
            "save E >> models/Seq2seq-Attention_E_iter200.pt, D >> models/Seq2seq-Attention_D_iter200.pt\n",
            "iter : 200\n",
            " - [input] I try\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-1e6b10012121>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss was saved to >> {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLEARNING_PROCESS_RESULT_LOSS_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-1e6b10012121>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iter :'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' - [input]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'corpus1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_Xs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' - [output]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'corpus2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mD_outputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' - [gt]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'corpus2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-1e6b10012121>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iter :'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' - [input]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'corpus1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_Xs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' - [output]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'corpus2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mD_outputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' - [gt]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'corpus2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADaElar6khEh",
        "colab_type": "text"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fjit-oNlkhEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test\n",
        "def test():\n",
        "    print('-' * 20)\n",
        "    print('test function')\n",
        "    print('-' * 20)\n",
        "    E = Encoder(cfg.CORPUS1_NUM).to(cfg.DEVICE)\n",
        "    D = Decoder(cfg.CORPUS2_NUM).to(cfg.DEVICE)\n",
        "    D.load_state_dict(torch.load(cfg.TEST.MODEL_D_PATH, map_location=torch.device(cfg.DEVICE)))\n",
        "    E.eval()\n",
        "    D.eval()\n",
        "\n",
        "    def generate(sentence):\n",
        "        corpus1 = data_dict['corpus1']\n",
        "        corpus2 = data_dict['corpus2']\n",
        "\n",
        "        Xs = [corpus1.index('<SOS>')]\n",
        "\n",
        "        for word in sentence.split(' '):\n",
        "            if word in corpus1:\n",
        "                Xs.append(corpus1.index(word))\n",
        "            else:\n",
        "                Xs.append(corpus1.index('<UNKNOWN>'))\n",
        "\n",
        "            # encode process\n",
        "            E_hidden = E.initHidden() # initialize encoder hidden\n",
        "            E_outputs = torch.zeros(cfg.SEQ2SEQ_MAX_LENGTH, cfg.SEQ2SEQ_RNN_DIM * (cfg.SEQ2SEQ_USE_RNN_BD + 1)).to(cfg.DEVICE)\n",
        "\n",
        "            for ei in range(xs_length):\n",
        "                E_output, E_hidden = E(Xs[ei], E_hidden, Xs)\n",
        "                E_outputs[ei] = E_output[0, 0]\n",
        "\n",
        "            # decode process\n",
        "            D_xs = ts[0].reshape(1, -1) # define decoder input\n",
        "            D_hidden = E_hidden # define decoder hidden\n",
        "            D_self_memory = D_xs\n",
        "            D_outputs = []\n",
        "\n",
        "            while 1:\n",
        "                # decode\n",
        "                D_ys, D_hidden = D(D_xs, D_hidden, E_outputs, D_self_memory)\n",
        "                \n",
        "                if cfg.SEQ2SEQ_NEXT_WORD_SELECTION == \"argmax\":\n",
        "                    topv, topi = D_ys.data.topk(1)\n",
        "\n",
        "                elif cfg.SEQ2SEQ_NEXT_WORD_SELECTION == \"prob\":\n",
        "                    topi = torch.multinomial(torch.max(D_ys, torch.zeros_like(D_ys) + 1e-5), 1)\n",
        "                \n",
        "                # define next decoder input\n",
        "                if use_teacher:\n",
        "                    D_xs = ts[di] # teacher forcing\n",
        "                else:\n",
        "                    D_xs = topi#.squeeze().detach()\n",
        "\n",
        "                D_xs = D_xs.reshape(1, -1)\n",
        "                D_self_memory = torch.cat([D_self_memory, D_xs])\n",
        "\n",
        "                D_outputs.append(topi.item())\n",
        "\n",
        "                if len(D_outputs) > cfg.SEQ2SEQ_MAX_LENGTH:\n",
        "                    break\n",
        "                if topi.item() == corpus2.index('<EOS>'):\n",
        "                    break\n",
        "\n",
        "            print(' - [input]', ' '.join([corpus1][x] for x in Xs[1:]]))\n",
        "            print(' - [output]', ' '.join([corpus2[x] for x in D_outputs if x not in [0, 1, 2]]))\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sen in ['I like apple', 'Go ahead', 'Thank you for your nice advice']:\n",
        "            generate(sen)\n",
        "\n",
        "test()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xGw8eZAkhEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def arg_parse():\n",
        "    parser = argparse.ArgumentParser(description='CNN implemented with Keras')\n",
        "    parser.add_argument('--train', dest='train', action='store_true')\n",
        "    parser.add_argument('--test', dest='test', action='store_true')\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "# main\n",
        "if __name__ == '__main__':\n",
        "    args = arg_parse()\n",
        "\n",
        "    if args.train:\n",
        "        train()\n",
        "    if args.test:\n",
        "        test()\n",
        "\n",
        "    if not (args.train or args.test):\n",
        "        print(\"please select train or test flag\")\n",
        "        print(\"train: python main.py --train\")\n",
        "        print(\"test:  python main.py --test\")\n",
        "        print(\"both:  python main.py --train --test\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0yDrCdGB3LJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "a = np.arange(3)\n",
        "b = np.arange(3) + 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VrqdgddB9f1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdpZjfJsCDyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY9qvN1ZCEOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.meshgrid(a, b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQHrmp3FCGIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c = np.zeros([10, 10])\n",
        "c[np.meshgrid(a, b)] = 1\n",
        "c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q58XGKqCQrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "True + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRE_b7PTOlNp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}